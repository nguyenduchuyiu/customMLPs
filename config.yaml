model:
  input_shape: [3, 32, 32]
  output_size: 10
  layers:
    - units: 1024
      activation: ReLU
      dropout_rate: 0.1
      batch_norm: true
    - units: 512
      activation: ReLU
      dropout_rate: 0.1
      batch_norm: true
    - units: 256
      activation: ReLU
      dropout_rate: 0.1
      batch_norm: true

optimizer:
  name: Adam
  learning_rate: 0.005
  weight_decay: 0.00001
  momentum: 0.9

training:
  batch_size: 128
  epochs: 10
  num_workers: 4
  pin_memory: true
  mixed_precision: true
  early_stopping_patience: 10
  scheduler_patience: 2
  scheduler_factor: 0.1

dataset: CIFAR10